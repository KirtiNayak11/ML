{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1vOdhgrC-LiaHXzKrUkrFb5pgKp5Zvlpg",
      "authorship_tag": "ABX9TyOO6ZwKshl/L8j+wXEXtsWS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KirtiNayak11/ML/blob/main/TextGenertion_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "\n",
        "# Load the IMDb dataset\n",
        "max_features = 10000  # Number of words to consider as features\n",
        "maxlen = 200  # Cut off reviews after 200 words\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to have consistent length\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# Build the RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))  # Embedding layer\n",
        "model.add(SimpleRNN(64))  # Simple RNN layer with 64 units\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuMCTAOov8OI",
        "outputId": "7aac3efd-316d-46fd-d9e5-2dcbcc630ff9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 1s 0us/step\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 32)          320000    \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 64)                6208      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 326,273\n",
            "Trainable params: 326,273\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "313/313 [==============================] - 38s 115ms/step - loss: 0.5652 - accuracy: 0.6865 - val_loss: 0.4107 - val_accuracy: 0.8272\n",
            "Epoch 2/5\n",
            "313/313 [==============================] - 25s 78ms/step - loss: 0.3515 - accuracy: 0.8534 - val_loss: 0.4178 - val_accuracy: 0.8078\n",
            "Epoch 3/5\n",
            "313/313 [==============================] - 23s 73ms/step - loss: 0.2283 - accuracy: 0.9089 - val_loss: 0.3873 - val_accuracy: 0.8472\n",
            "Epoch 4/5\n",
            "313/313 [==============================] - 25s 80ms/step - loss: 0.1302 - accuracy: 0.9530 - val_loss: 0.4746 - val_accuracy: 0.8236\n",
            "Epoch 5/5\n",
            "313/313 [==============================] - 25s 79ms/step - loss: 0.0830 - accuracy: 0.9714 - val_loss: 0.5580 - val_accuracy: 0.8080\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.5939 - accuracy: 0.8044\n",
            "Test Loss: 0.5939280986785889\n",
            "Test Accuracy: 0.8043599724769592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_reviews = [\n",
        "    \"This movie was fantastic! I loved every moment of it.\",\n",
        "    \"The acting was terrible and the plot made no sense.\",\n",
        "    \"I couldn't stop laughing throughout the whole film.\",\n",
        "    \"The cinematography was stunning, but the story was lacking.\",\n",
        "]\n",
        "\n",
        "# Convert text to sequences and pad\n",
        "sample_sequences = []\n",
        "for review in sample_reviews:\n",
        "    sequence = [imdb.get_word_index().get(word.lower(), 0) for word in review.split()]\n",
        "    sample_sequences.append(sequence)\n",
        "\n",
        "sample_sequences = pad_sequences(sample_sequences, maxlen=maxlen)\n",
        "\n",
        "# Predict sentiment\n",
        "predictions = model.predict(sample_sequences)\n",
        "\n",
        "for review, prediction in zip(sample_reviews, predictions):\n",
        "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
        "    print(f\"Review: {review}\\nPredicted Sentiment: {sentiment}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1HptT3HwClS",
        "outputId": "24111df1-d3d6-4156-d603-7c3d407958f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 1s 0us/step\n",
            "1/1 [==============================] - 0s 188ms/step\n",
            "Review: This movie was fantastic! I loved every moment of it.\n",
            "Predicted Sentiment: Negative\n",
            "\n",
            "Review: The acting was terrible and the plot made no sense.\n",
            "Predicted Sentiment: Negative\n",
            "\n",
            "Review: I couldn't stop laughing throughout the whole film.\n",
            "Predicted Sentiment: Negative\n",
            "\n",
            "Review: The cinematography was stunning, but the story was lacking.\n",
            "Predicted Sentiment: Negative\n",
            "\n"
          ]
        }
      ]
    }
  ]
}